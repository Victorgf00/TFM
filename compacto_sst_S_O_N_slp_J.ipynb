{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b74c13",
   "metadata": {},
   "source": [
    "# Compact notebook with the implementation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b4680c-a24d-419d-a34d-5f9c2017b72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#En primer lugar, vamos a importar todos los paquetes que vamos a usar a lo largo del examen\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "#import netCDF4 as nc\n",
    "from scipy import stats as sts\n",
    "import scipy.stats as stats\n",
    "from scipy import signal\n",
    "from scipy.fft import fft \n",
    "#from statsmodels.tsa import stattools \n",
    "from cartopy import crs as ccrs # Cartography library\n",
    "import cartopy as car\n",
    "#import cmocean as cmo #Oceanographic library for colormaps\n",
    "#import plotly as plty\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import numpy.linalg as linalg\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.dates as mdates\n",
    "from cartopy.util import add_cyclic_point #esto es para la banda de latitud que queda en blanco en 0Âª\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "#The following two lines are coded to avoid the warning unharmful message.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28c964f4",
   "metadata": {},
   "source": [
    "## * Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a0ac14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_slp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m had_sst_regrid,lat_sst,lon_sst \u001b[39m=\u001b[39m data_mining(\u001b[39m'\u001b[39m\u001b[39mDatasets/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mpath_had_sst, lat_lims_sst, lon_lims_sst, time_lims, scale\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, rotate_360\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, regrid\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, regrid_degree\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,overlapping\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,variable_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msst\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m data_sst,anom_sst,norm_sst,mean_sst,std_sst\u001b[39m=\u001b[39m data_selection(had_sst_regrid, months_sst, years,months_skip_sst,years_finally_slp,reference_period,mean_months\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m X_train,X_valid,X_test,Y_train,Y_valid,Y_test\u001b[39m=\u001b[39m data_split(train_years,validation_years,testing_years,norm_sst,norm_slp, predictor_nans\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, predictant_nans\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'norm_slp' is not defined"
     ]
    }
   ],
   "source": [
    "had_sst_regrid,lat_sst,lon_sst = data_mining('Datasets/'+path_had_sst, lat_lims_sst, lon_lims_sst, time_lims, scale=1, rotate_360=True, regrid=False, regrid_degree=2,overlapping=True,variable_name='sst')\n",
    "data_sst,anom_sst,norm_sst,mean_sst,std_sst= data_selection(had_sst_regrid, months_sst, years,months_skip_sst,years_finally_slp,reference_period,mean_months=False)\n",
    "X_train,X_valid,X_test,Y_train,Y_valid,Y_test= data_split(train_years,validation_years,testing_years,norm_sst,norm_slp, predictor_nans=True, predictant_nans=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e01425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_selection(data, months, years, months_to_drop,years_out,reference_period,mean_months=False):\n",
    "    data_red = data.sel(time=slice(f\"{years[0]}\", f\"{years[-1]}\"))\n",
    "    data_red = data_red.sel(time=np.isin(data['time.month'], months))\n",
    "    data_red = data_red.drop_sel(time=months_to_drop)\n",
    "    if mean_months:\n",
    "        data_red = data_red.groupby('time.month')\n",
    "        mean_data= 0\n",
    "        for i in months:\n",
    "            mean_data += np.array(data_red[i])\n",
    "        mean_data /=len(months)\n",
    "        data_red = xr.DataArray(data=mean_data,dims=[\"year\", \"latitude\", \"longitude\"], \n",
    "                            coords=dict(longitude=([\"longitude\"], np.array(data.longitude)),\n",
    "                                        latitude=([\"latitude\"], np.array(data.latitude)),year=years_out))\n",
    "        \n",
    "    mean_reference= (data_red.sel(time=slice(str(reference_period[0]),str(reference_period[1])))).mean(dim='time')\n",
    "    std_reference= (data_red.sel(time=slice(str(reference_period[0]),str(reference_period[1])))).std(dim='time')\n",
    "    anomaly= data_red-mean_reference\n",
    "    normalization= anomaly/std_reference\n",
    "    return data_red,anomaly,normalization,mean_reference,std_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "999f0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_selection(data, months, years, months_to_drop, years_out, reference_period, mean_months=False):\n",
    "    data_red = data.sel(time=slice(f\"{years[0]}\", f\"{years[-1]}\"))\n",
    "    data_red = data_red.sel(time=np.isin(data['time.month'], months))\n",
    "    data_red = data_red.drop_sel(time=months_to_drop)\n",
    "    if mean_months:\n",
    "        data_red = data_red.groupby('time.month')\n",
    "        mean_data = 0\n",
    "        for i, month_group in data_red:\n",
    "            month_data = data.sel(time=month_group.time)\n",
    "            month_ref_data = month_data.sel(time=slice(str(reference_period[i-1][0]), str(reference_period[i-1][1]))).mean(dim='time')\n",
    "            mean_data += month_ref_data\n",
    "        mean_data /= len(months)\n",
    "        print(mean_data.shape)\n",
    "        print(years_out.shape)\n",
    "        data_red = xr.DataArray(data=mean_data, dims=[\"year\", \"latitude\", \"longitude\"], \n",
    "                            coords=dict(longitude=([\"longitude\"], np.array(data.longitude)),\n",
    "                                        latitude=([\"latitude\"], np.array(data.latitude)),year=years_out))\n",
    "    else:\n",
    "        month_means = []\n",
    "        month_stds = []\n",
    "        for i in months:\n",
    "            month_data = data_red.sel(time=np.isin(data_red['time.month'], i))\n",
    "            month_ref_data = month_data.sel(time=slice(str(reference_period[i-1][0]),str(reference_period[i-1][1]))).mean(dim='time')\n",
    "            month_means.append(month_ref_data)\n",
    "            month_ref_std = month_data.sel(time=slice(str(reference_period[i-1][0]),str(reference_period[i-1][1]))).std(dim='time')\n",
    "            month_stds.append(month_ref_std)\n",
    "        data_red_mean = xr.concat(month_means, dim='time').mean(dim='time')\n",
    "        data_red_std = xr.concat(month_stds, dim='time').mean(dim='time')\n",
    "        data_red = xr.DataArray(data=data_red_mean, dims=[\"year\", \"latitude\", \"longitude\"], \n",
    "                            coords=dict(longitude=([\"longitude\"], np.array(data.longitude)),\n",
    "                                        latitude=([\"latitude\"], np.array(data.latitude)),year=years_out))\n",
    "    anomaly = data_red - data_red_mean\n",
    "    normalization = anomaly / data_red_std\n",
    "    return data_red, anomaly, normalization, data_red_mean, data_red_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mining(relative_path,lat_lims,lon_lims,time_lims, scale=1,rotate_180=False,rotate_360=False,regrid=False,regrid_degree=2,overlapping=False,variable_name=None,rename=False,latitude_regrid=False):\n",
    "    data= xr.open_dataset(relative_path)/scale\n",
    "    time = data['time'].astype('datetime64[M]') #esto es para facilitar el trato con los datos\n",
    "    data = data.assign_coords(time=time)\n",
    "    if rename:\n",
    "        data= data.rename({'lat':'latitude','lon':'longitude'})\n",
    "    if latitude_regrid:\n",
    "        lat_newgrid= np.arange(-90,91,1) \n",
    "        data= data.interp(latitude=np.array(lat_newgrid))\n",
    "        data= data.sortby('latitude',ascending=False)\n",
    "    if rotate_180:\n",
    "        data= data.assign_coords(longitude=(((data.longitude + 180) % 360) - 180)).sortby('longitude')\n",
    "    if rotate_360:\n",
    "        data= data.assign_coords(longitude=np.where(data.longitude < 0,360+(data.longitude),data.longitude)).sortby('longitude')\n",
    "    data= data.sel(latitude= slice(lat_lims[0],lat_lims[1]),longitude= slice(lon_lims[0],lon_lims[1]),time=slice(str(time_lims[0]),str(time_lims[1])))\n",
    "    if regrid:\n",
    "        lon_regrid= np.arange(lon_lims[0],lon_lims[1],regrid_degree)\n",
    "        lat_regrid= np.arange(lat_lims[0],lat_lims[1],-regrid_degree) \n",
    "        data= data.interp(longitude=np.array(lon_regrid)).interp(latitude=np.array(lat_regrid))\n",
    "    latitude= data.latitude\n",
    "    longitude= data.longitude\n",
    "    data= data[str(variable_name)]\n",
    "    if overlapping:\n",
    "        creg, longitude = add_cyclic_point(np.array(data), coord=longitude)       \n",
    "        data = xr.DataArray(\n",
    "            data=creg,\n",
    "            dims=[\"time\", \"latitude\", \"longitude\"],\n",
    "            coords=dict(\n",
    "                longitude=([\"longitude\"], np.array(longitude)),\n",
    "                latitude=([\"latitude\"], np.array(latitude)),\n",
    "                time=np.array(data.time)))\n",
    "    return data,latitude,longitude\n",
    "\n",
    "def data_selection(data, months, years,months_to_drop,years_out,reference_period,drop_months=False,):\n",
    "    data_red = data.sel(time=slice(f\"{years[0]}\", f\"{years[-1]}\"))\n",
    "    data_red = data_red.sel(time=np.isin(data['time.month'], months))\n",
    "    if drop_months==True:\n",
    "        data_red = data_red.drop_sel(time=months_to_drop)\n",
    "    data_red = data_red.groupby('time.month')\n",
    "    mean_data= 0\n",
    "    for i in months:\n",
    "        mean_data += np.array(data_red[i])\n",
    "    mean_data /=len(months)\n",
    "    data_red = xr.DataArray(data=mean_data,dims=[\"year\", \"latitude\", \"longitude\"], \n",
    "                            coords=dict(longitude=([\"longitude\"], np.array(data.longitude)),\n",
    "                                        latitude=([\"latitude\"], np.array(data.latitude)),year=years_out))\n",
    "    \n",
    "    mean_reference= (data_red.sel(year=slice(str(reference_period[0]),str(reference_period[1])))).mean(dim='year')\n",
    "    std_reference= (data_red.sel(year=slice(str(reference_period[0]),str(reference_period[1])))).std(dim='year')\n",
    "    anomaly= data_red-mean_reference\n",
    "    normalization= anomaly/std_reference\n",
    "    return data_red,anomaly,normalization,mean_reference,std_reference\n",
    "\n",
    "def dibujo_1_mapa_cartopy(A,lon,lat,levs,cmap1,l1,titulo, ax):\n",
    "    im=ax.contourf(lon,lat,A\n",
    "                   ,cmap=cmap1,levels=levs,extend='both',transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(linewidth=0.75)\n",
    "    ax.set_title(titulo,fontsize=28)\n",
    "    cbar = plt.colorbar(im, extend='neither', spacing='proportional',\n",
    "                orientation='vertical', shrink=0.7, format=\"%2.2f\")\n",
    "    cbar.set_label(l1, size=15)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "def dibujo_1_mapa_cartopy_pixel(A, lon, lat, levs, cmap1, l1, titulo, ax):\n",
    "    # Define the levels you want\n",
    "    num_levels = len(levs)\n",
    "    levels = np.linspace(-1, 1, num_levels)\n",
    "    # Create a BoundaryNorm object\n",
    "    cmap1 = plt.cm.get_cmap(cmap1)\n",
    "    norm = colors.BoundaryNorm(levels, ncolors=cmap1.N, clip=True)\n",
    "    im = ax.pcolormesh(lon,lat,A,cmap=cmap1, transform =ccrs.PlateCarree(), norm= norm) # norm es para los definir los niveles que queremos\n",
    "    ax.coastlines(linewidth=0.75)\n",
    "    ax.set_title(titulo, fontsize=28)\n",
    "    cbar = plt.colorbar(im, extend='neither', spacing='proportional',orientation='vertical', shrink=0.7, format=\"%2.2f\")\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    cbar.set_label(l1, size=15)\n",
    "\n",
    "def data_split(train_years,validation_years,test_years,predictor,predictant, predictor_nans=False, predictant_nans=False):\n",
    "    \"\"\"\n",
    "    Prepares the data for training the model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_years : list\n",
    "        list containing the start and end years of the training data\n",
    "    validation_years : list\n",
    "        list containing the start and end years of the validation data\n",
    "    testing_years : list\n",
    "        list containing the start and end years of the test data\n",
    "    predictor : xarray datarray\n",
    "        dataarray containing the predictor data\n",
    "    predictant : xarray datarray\n",
    "        dataarray containing the predictant data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train : numpy array\n",
    "        numpy array containing the cleaned SST data for the training data\n",
    "    X_valid : numpy array\n",
    "        numpy array containing the cleaned SST data for the validation data\n",
    "    X_test : numpy array\n",
    "        numpy array containing the cleaned SST data for the test data\n",
    "    Y_train : xarray DataArray\n",
    "        xarray DataArray containing the SLP data for the training data\n",
    "    Y_valid : xarray DataArray\n",
    "        xarray DataArray containing the SLP data for the validation data\n",
    "    Y_test : xarray DataArray\n",
    "        xarray DataArray containing the SLP data for the test data\n",
    "    \"\"\"\n",
    "    X_train= predictor.sel(year=slice(train_years[0],train_years[1]))\n",
    "    X_valid= predictor.sel(year=slice(validation_years[0],validation_years[1]))\n",
    "    X_test= predictor.sel(year=slice(testing_years[0],testing_years[1]))\n",
    "\n",
    "    Y_train= predictant.sel(year=slice(train_years[0],train_years[1]))\n",
    "    Y_valid= predictant.sel(year=slice(validation_years[0],validation_years[1]))\n",
    "    Y_test= predictant.sel(year=slice(testing_years[0],testing_years[1]))\n",
    "\n",
    "    #since our data has nans, we must remove it to avoid problems when training the model\n",
    "    def quitonans(mat):\n",
    "        out = mat[:,~np.isnan(mat.mean(axis = 0))]\n",
    "        return out\n",
    "    \n",
    "    if predictor_nans==True:\n",
    "        nt_train,nlat,nlon= X_train.shape\n",
    "        nt_valid,nlat,nlon= X_valid.shape\n",
    "        nt_test,nlat,nlon= X_test.shape\n",
    "\n",
    "        X_train_reshape= np.reshape(np.array(X_train), (nt_train, nlat*nlon))\n",
    "        X_valid_reshape= np.reshape(np.array(X_valid), (nt_valid, nlat*nlon))\n",
    "        X_test_reshape= np.reshape(np.array(X_test), (nt_test, nlat*nlon))\n",
    "\n",
    "        X_train= quitonans(X_train_reshape)\n",
    "        X_valid= quitonans(X_valid_reshape)\n",
    "        X_test= quitonans(X_test_reshape)\n",
    "    if predictant_nans==True:\n",
    "        nt_train,nlat,nlon= Y_train.shape\n",
    "        nt_valid,nlat,nlon= Y_valid.shape\n",
    "        nt_test,nlat,nlon= Y_test.shape\n",
    "\n",
    "        Y_train_reshape= np.reshape(np.array(Y_train), (nt_train, nlat*nlon))\n",
    "        Y_valid_reshape= np.reshape(np.array(Y_valid), (nt_valid, nlat*nlon))\n",
    "        Y_test_reshape= np.reshape(np.array(Y_test), (nt_test, nlat*nlon))\n",
    "\n",
    "        Y_train= quitonans(Y_train_reshape)\n",
    "        Y_valid= quitonans(Y_valid_reshape)\n",
    "        Y_test= quitonans(Y_test_reshape)\n",
    "    return X_train,X_valid,X_test,Y_train,Y_valid,Y_test\n",
    "\n",
    "def create_model(input_shape,output_shape, layer_sizes, activations, dropout_rates, kernel_regularizer=None):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        model.add(tf.keras.layers.Dense(units=layer_sizes[i], activation=activations[i]))\n",
    "        if i < len(dropout_rates):\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rates[i]))\n",
    "    model.add(tf.keras.layers.Dense(units=layer_sizes[-1],activation=activations[-1] ,kernel_regularizer= kernel_regularizer))\n",
    "    model.add(tf.keras.layers.Dense(units=output_shape[0]*output_shape[1]))\n",
    "    model.add(tf.keras.layers.Reshape(output_shape))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def performance_plot(record1):\n",
    "    plt.style.use('seaborn')\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    fig1.suptitle('Models Performance')\n",
    "    ax1.plot(record1.history['loss'])\n",
    "    ax1.plot(record1.history['val_loss'])\n",
    "    ax1.set_title('Model 1')\n",
    "    ax1.set_xlabel('# Epochs')\n",
    "    ax1.set_ylabel('Loss magnitude')\n",
    "    ax1.legend(['Training', 'Validation'])\n",
    "\n",
    "def evaluation(model,X_test,Y_test,longitude,latitude,std_slp):\n",
    "    predicted1= model.predict(np.array(X_test))\n",
    "    print(predicted1.shape)\n",
    "    results1 = model.evaluate(np.array(X_test), np.array(Y_test), batch_size=32)\n",
    "\n",
    "    predicted1 = xr.DataArray(\n",
    "        data=predicted1,\n",
    "        dims=[\"time\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], np.array(longitude)),\n",
    "            latitude=([\"latitude\"], np.array(latitude)),\n",
    "            time=np.array(Y_test.year)))\n",
    "\n",
    "    #las desestandarizamos para quedarnos con anomalÃ­as\n",
    "    predicted1= predicted1*std_slp\n",
    "    correct_value= Y_test*std_slp\n",
    "    return predicted1,correct_value\n",
    "\n",
    "def DibujoMapados(lon,lat,lon0,lat0,var0,var1,var2,mapbar,unidades,titulo1,titulo2,subtitulo1,periodo,subtitulo2,figura,aÃ±o):\n",
    "    plt.style.use('seaborn')\n",
    "    fig0 = plt.figure(figsize=(10,7.5))\n",
    "    ax1 = fig0.add_subplot(221, projection=ccrs.PlateCarree())\n",
    "    ax2 = fig0.add_subplot(222, projection=ccrs.PlateCarree())\n",
    "    ax0 = fig0.add_subplot(212, projection=ccrs.PlateCarree())\n",
    "    axes = [ax1, ax2, ax0]\n",
    "    fig0.suptitle(str(titulo1)+' for '+str(aÃ±o)+\n",
    "                 '\\n Reference period: '+str(reference_period),fontsize=15, weight='bold')\n",
    "\n",
    "    im0=ax0.contourf(lon0,lat0,var0,levels=np.round(np.linspace(-2.5,2.5,20),decimals=1), cmap = 'RdBu_r', extend='both')\n",
    "    ax0.coastlines(linewidth = 2)\n",
    "    gl=ax0.gridlines(draw_labels = True)\n",
    "    gl.ylabels_right = False\n",
    "    gl.xlabels_top = False\n",
    "    ax0.set_title(str(titulo2)+' for '+str(aÃ±o)+' '+str(periodo),fontsize=15)\n",
    "    maximo1= np.max((np.ma.masked_array(var1, np.isnan(var1))))\n",
    "    maximo2= np.max((np.ma.masked_array(var2, np.isnan(var2))))\n",
    "    maximo= np.max((maximo1,maximo2))\n",
    "    minimo1= np.min((np.ma.masked_array(var1, np.isnan(var1))))\n",
    "    minimo2= np.min((np.ma.masked_array(var2, np.isnan(var2))))\n",
    "    minimo= np.min((minimo1,minimo2))\n",
    "    rango= np.max((np.abs(maximo),np.abs(minimo)))\n",
    "    \n",
    "    #im=ax1.contourf(lon,lat,var1,levels=np.round(np.linspace(-rango,rango,20),decimals=1), cmap = mapbar, extend='both')\n",
    "    im = ax1.pcolormesh(lon,lat,var1,cmap=mapbar,vmin=-rango, vmax=rango, transform =ccrs.PlateCarree()) # En transform se pone la proyecciÃ³n default en la que estÃ¡n los datos \n",
    "    ax1.coastlines(linewidth = 2)\n",
    "    gl=ax1.gridlines(draw_labels = True)\n",
    "    gl.ylabels_right = False\n",
    "    gl.xlabels_top = False\n",
    "\n",
    "    ax1.set_title(subtitulo1,fontsize=15)\n",
    "        \n",
    "    #im3=ax2.contourf(lon,lat,var2,levels=np.round(np.linspace(-rango,rango,20),decimals=1), cmap = mapbar, extend='both')\n",
    "    im3 = ax2.pcolormesh(lon,lat,var2,cmap=mapbar,vmin=-rango, vmax=rango, transform =ccrs.PlateCarree()) # En transform se pone la proyecciÃ³n default en la que estÃ¡n los datos \n",
    "    ax2.coastlines(linewidth = 2)\n",
    "    gl=ax2.gridlines(draw_labels = True)\n",
    "    gl.ylabels_right = False\n",
    "    gl.ylabels_left = False\n",
    "    gl.xlabels_top = False\n",
    "    #fig0.colorbar(im3,ax=[ax1,ax2], location='right',shrink=1, label = unidades,orientation='vertical')\n",
    "\n",
    "    fig0.subplots_adjust(right=1.)\n",
    "    fig0.colorbar(im0,ax=ax0,shrink=0.8, label ='$^\\circ$C',orientation='horizontal')\n",
    "\n",
    "    cbar_ax = fig0.add_axes([0.97, 0.475, 0.03, 0.4]) #left, bottom, width, height\n",
    "    fig0.colorbar(im3, cax=cbar_ax, label=unidades)\n",
    "\n",
    "    ax2.set_title(subtitulo2,fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    fig0.savefig(figura)\n",
    "\n",
    "def correlations(predicted1,correct_value,outputs_path,lat_iceland,lon_iceland,lat_macaronesian,lon_macaronesian,time_series=False,index_1=None,index_1_name=None,index_2=None,index_2_name=None):\n",
    "    predicted1,correct_value= evaluation(model1,X_test,Y_test,lon_slp,lat_slp,std_slp)\n",
    "    #First, let's take the test set with its predictions, and compare with the real values\n",
    "    predictions= predicted1\n",
    "    observations= correct_value.rename({'year':'time'})\n",
    "    spatial_correlation= xr.corr(predictions,observations,dim='time')\n",
    "    p_value= xs.pearson_r_p_value(predictions, observations, dim='time')\n",
    "    thresh = 0.1  # Set the significance threshold\n",
    "\n",
    "    # Plot significant pixels with point hatching\n",
    "    skill = ma.masked_where(p_value>0.1,spatial_correlation)\n",
    "\n",
    "    temporal_correlation= xr.corr(predictions,observations, dim=('longitude','latitude'))\n",
    "    temporal_correlation_iceland= xr.corr(predictions.sel(latitude=slice(lat_iceland[0],lat_iceland[1]),longitude=slice(lon_iceland[0],lon_iceland[1])),\n",
    "                                            observations.sel(latitude=slice(lat_iceland[0],lat_iceland[1]),longitude=slice(lon_iceland[0],lon_iceland[1])), dim=('longitude','latitude'))\n",
    "    temporal_correlation_macaronesian= xr.corr(predictions.sel(latitude=slice(lat_macaronesian[0],lat_macaronesian[1]),longitude=slice(lon_macaronesian[0],lon_macaronesian[1])),\n",
    "                                                observations.sel(latitude=slice(lat_macaronesian[0],lat_macaronesian[1]),longitude=slice(lon_macaronesian[0],lon_macaronesian[1])), dim=('longitude','latitude'))\n",
    "    spatial_rmse = np.sqrt(((predictions - observations) ** 2).mean(dim='time'))\n",
    "    temporal_rmse= np.sqrt(((predictions - observations) ** 2).mean(dim=('longitude','latitude')))\n",
    "    temporal_rmse_iceland= np.sqrt((((predictions.sel(latitude=slice(lat_iceland[0],lat_iceland[1]),longitude=slice(lon_iceland[0],lon_iceland[1])))-\n",
    "                                            observations.sel(latitude=slice(lat_iceland[0],lat_iceland[1]),longitude=slice(lon_iceland[0],lon_iceland[1])))**2).mean(dim=('longitude','latitude')))\n",
    "    temporal_rmse_macaronesian= np.sqrt((((predictions.sel(latitude=slice(lat_macaronesian[0],lat_macaronesian[1]),longitude=slice(lon_macaronesian[0],lon_macaronesian[1])))-\n",
    "                                                observations.sel(latitude=slice(lat_macaronesian[0],lat_macaronesian[1]),longitude=slice(lon_macaronesian[0],lon_macaronesian[1])))**2).mean(dim=('longitude','latitude')))\n",
    "\n",
    "    thresh = 0.1  # Set the significance threshold\n",
    "    sig_pixels = np.abs(p_value) <= thresh  # Boolean array of significant pixels\n",
    "\n",
    "    # Set non-significant pixels to NaN\n",
    "    spatial_correlation_sig = spatial_correlation.where(sig_pixels)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(15,7)) \n",
    "    ax = fig.add_subplot(221, projection=ccrs.PlateCarree(0))\n",
    "    data= spatial_correlation\n",
    "    rango= 1\n",
    "    dibujo_1_mapa_cartopy_pixel(data,data.longitude,data.latitude,np.linspace(-rango,+rango,15),'bwr','Correlation','Temporal Correlation', ax)\n",
    "    #ax.pcolor(data.longitude, data.latitude, skill, hatch='.', alpha=0.)\n",
    "    lon_sig, lat_sig= spatial_correlation_sig.stack(pixel=('longitude', 'latitude')).dropna('pixel').longitude, spatial_correlation_sig.stack(pixel=('longitude', 'latitude')).dropna('pixel').latitude\n",
    "\n",
    "    ax.scatter(lon_sig, lat_sig, s=5, c='k', marker='.', alpha=0.5, transform=ccrs.PlateCarree(), label='Significant')\n",
    "\n",
    "    rect = mpatches.Rectangle((lon_iceland[0], lat_iceland[0]), lon_iceland[1]-lon_iceland[0], lat_iceland[1]-lat_iceland[0], linewidth=2, edgecolor='green', facecolor='none',fill=True, transform=ccrs.PlateCarree(),label='iceland region')\n",
    "    ax.add_patch(rect)\n",
    "    rect = mpatches.Rectangle((lon_macaronesian[0], lat_macaronesian[0]), lon_macaronesian[1]-lon_macaronesian[0], lat_macaronesian[1]-lat_macaronesian[0], linewidth=2, edgecolor='orange', facecolor='none',fill=True, transform=ccrs.PlateCarree(),label='Macaronesian region')\n",
    "    ax.add_patch(rect)\n",
    "    ax.legend(loc='best',facecolor=\"white\")\n",
    "\n",
    "    plt.style.use('seaborn')\n",
    "    ax1 = fig.add_subplot(222)\n",
    "    data = {'time': temporal_correlation.time,'Predictions correlation': temporal_correlation,'Iceland correlation': temporal_correlation_iceland,'Macaronesian correlation': temporal_correlation_macaronesian}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('time', inplace=True)\n",
    "    color_dict = {'Predictions correlation': 'blue', \n",
    "                'Iceland correlation': 'green', \n",
    "                'Macaronesian correlation': 'orange'}\n",
    "    # Set the width of each bar\n",
    "    width = 0.25\n",
    "    # Plot the bars using the bar method\n",
    "    for i, col in enumerate(df.columns):\n",
    "        ax1.bar(df.index - width + (i * width), df[col], width=width, color=color_dict[col], label=col)\n",
    "    # Create ax2 and ax3\n",
    "    if time_series:\n",
    "        ax2 = ax1.twinx()\n",
    "        ax3 = ax1.twinx()\n",
    "\n",
    "        # Position ax2 and ax3 on the right side of the figure\n",
    "        ax2.spines[\"right\"].set_position((\"axes\", 1.))\n",
    "        ax3.spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "\n",
    "        # Plot the curves on ax2 and ax3\n",
    "        ax2.plot(temporal_correlation.time, index_1, label=str(index_1_name), color='purple')\n",
    "        ax3.plot(temporal_correlation.time, index_2, label=str(index_2_name), color='red')\n",
    "\n",
    "        # Set the limits and labels for the y-axes on ax2 and ax3\n",
    "        ax2.set_ylim(ymin=-3, ymax=+3)\n",
    "        ax2.set_ylabel(str(index_1_name), color='purple')\n",
    "        ax2.tick_params(axis='y', labelcolor='purple')\n",
    "        ax3.set_ylim(ymin=-1.5, ymax=+1.5)\n",
    "        ax3.set_ylabel(str(index_2_name), color='red')\n",
    "        ax3.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        # Add legends for the curves on ax2 and ax3\n",
    "        ax2.legend(loc='upper left')\n",
    "        ax3.legend(loc='lower left')\n",
    "\n",
    "    ax1.set_ylim(ymin=-1,ymax=+1)\n",
    "    ax1.set_title('Spatial Correlation')\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    ax4 = fig.add_subplot(223, projection=ccrs.PlateCarree(0))\n",
    "    data= spatial_rmse\n",
    "    dibujo_1_mapa_cartopy(data,data.longitude,data.latitude,np.linspace(0,np.max(np.array(data)),20),'OrRd','RMSE','Temporal RMSE', ax4)\n",
    "    rect = mpatches.Rectangle((lon_iceland[0], lat_iceland[0]), lon_iceland[1]-lon_iceland[0], lat_iceland[1]-lat_iceland[0], linewidth=2, edgecolor='green', facecolor='none',fill=True, transform=ccrs.PlateCarree(),label='iceland region')\n",
    "    ax4.add_patch(rect)\n",
    "    rect = mpatches.Rectangle((lon_macaronesian[0], lat_macaronesian[0]), lon_macaronesian[1]-lon_macaronesian[0], lat_macaronesian[1]-lat_macaronesian[0], linewidth=2, edgecolor='orange', facecolor='none',fill=True, transform=ccrs.PlateCarree(),label='Macaronesian region')\n",
    "    ax4.add_patch(rect)\n",
    "    ax4.legend(loc='best',facecolor=\"white\")\n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    ax5 = fig.add_subplot(224)\n",
    "    data = {'time': temporal_rmse.time,'Predictions RMSE': temporal_rmse,'Iceland RMSE': temporal_rmse_iceland,'Macaronesian RMSE': temporal_rmse_macaronesian}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('time', inplace=True)\n",
    "    color_dict = {'Predictions RMSE': 'blue', \n",
    "                'Iceland RMSE': 'green', \n",
    "                'Macaronesian RMSE': 'orange'}\n",
    "    # Set the width of each bar\n",
    "    width = 0.25\n",
    "    # Plot the bars using the bar method\n",
    "    for i, col in enumerate(df.columns):\n",
    "        ax5.bar(df.index - width + (i * width), df[col], width=width, color=color_dict[col], label=col)\n",
    "    # Create ax2 and ax3\n",
    "    if time_series:\n",
    "        ax6 = ax5.twinx()\n",
    "        ax7 = ax5.twinx()\n",
    "\n",
    "        # Position ax2 and ax3 on the right side of the figure\n",
    "        ax6.spines[\"right\"].set_position((\"axes\", 1.))\n",
    "        ax7.spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "\n",
    "        # Plot the curves on ax2 and ax3\n",
    "        ax6.plot(temporal_rmse.time, index_1, label=str(index_1_name), color='purple')\n",
    "        ax7.plot(temporal_rmse.time, index_2, label=str(index_2_name), color='red')\n",
    "\n",
    "        # Set the limits and labels for the y-axes on ax2 and ax3\n",
    "        ax6.set_ylim(ymin=-3, ymax=+3)\n",
    "        ax6.set_ylabel(str(index_1_name), color='purple')\n",
    "        ax6.tick_params(axis='y', labelcolor='purple')\n",
    "        ax7.set_ylim(ymin=-1.5, ymax=+1.5)\n",
    "        ax7.set_ylabel(str(index_2_name), color='red')\n",
    "        ax7.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        # Add legends for the curves on ax2 and ax3\n",
    "        ax6.legend(loc='upper left')\n",
    "        ax7.legend(loc='lower left')\n",
    "\n",
    "    #ax5.set_ylim(ymin=-1,ymax=+1)\n",
    "    ax5.set_title('Spatial RMSE')\n",
    "    ax5.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outputs_path+'correlations')\n",
    "\n",
    "def nino_index(lon_index,lat_index,data,time_period):\n",
    "    data_region= data.sel(latitude=slice(lat_index[0],lat_index[1]),longitude=slice(lon_index[0],lon_index[1]),time=slice(str(time_period[0]),str(time_period[1])))\n",
    "    #calculamos la media espacial de SST para cada tiempo y calculamos la anomalÃ­a\n",
    "    spatial_mean= data_region.mean(dim=('latitude','longitude'))\n",
    "    anomaly= ((spatial_mean.groupby('time.month')-spatial_mean.groupby('time.month').mean('time'))).groupby('time.month')/spatial_mean.groupby('time.month').std('time')\n",
    "    index= ((anomaly.groupby('time.season')['SON']).groupby('time.year')).mean()\n",
    "    return index\n",
    "\n",
    "def nao_index(azores_location, iceland_location,data,time_period):\n",
    "    escala= 1/100 #to convert Pa to hPa\n",
    "    data= data.sel(time=slice(str(time_period[0]),str(time_period[1])))\n",
    "    Azores_point= data.sel(latitude=Azores[0],longitude=Azores[1],method='nearest')*escala\n",
    "    icelandia_point= data.sel(latitude=icelandia[0],longitude=icelandia[1],method='nearest')*escala\n",
    "    diferencia_presion= Azores_point-icelandia_point\n",
    "    NAO=((diferencia_presion.groupby('time.month')-diferencia_presion.groupby('time.month').mean('time'))).groupby('time.month')/diferencia_presion.groupby('time.month').std('time')\n",
    "    return NAO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b85aea68",
   "metadata": {},
   "source": [
    "## * Hyperparameters & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b04d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'C:/Users/ideapad 5 15ITL05/Desktop/TFM/Notebooks/Datasets/'\n",
    "path_era_slp= 'slp_ERA20_1900-2010.nc'\n",
    "path_had_sst= 'HadISST1_sst_1870-2019.nc'\n",
    "time_lims= [1900,2010]\n",
    "lat_lims_slp, lon_lims_slp= [70,20], [-40,30]\n",
    "lat_lims_sst, lon_lims_sst= [20,-20], [0,360]\n",
    "months_slp= [12,1,2]\n",
    "years= np.arange(1900,2011,1)\n",
    "months_skip_slp= ['1900-01','1900-02','2010-12']\n",
    "years_finally_slp= np.arange(1900,2010,1)\n",
    "reference_period= [1950,2000]\n",
    "months_sst= [9,10,11]\n",
    "months_skip_sst= ['2010-09','2010-10','2010-11']\n",
    "train_years, validation_years, testing_years= [1900,1985], [1985,1995], [1995,2010]\n",
    "layer_sizes = [32,8,64]\n",
    "activations = [tf.keras.activations.elu,tf.keras.activations.elu, tf.keras.activations.elu]\n",
    "dropout_rates = [0.05]\n",
    "kernel_regularizer = 'l1_l2'\n",
    "learning_rate, epochs= 0.0001,5000\n",
    "mapbar='bwr'\n",
    "titulo1= 'Comparison of DJF anomalies SLP'\n",
    "titulo2= 'SST anomaly'\n",
    "subtitulo1='Predictions'\n",
    "periodo= 'SON'\n",
    "subtitulo2='Observations'\n",
    "unidades='hPa'\n",
    "lon_nino3, lat_nino3= [-150,-90], [5,-5]\n",
    "lon_iceland, lat_iceland= [-40,-20], [70,60]\n",
    "lon_macaronesian, lat_macaronesian= [-40,-20], [40,20]\n",
    "Azores, icelandia= [35, 335], [64.136319, 360-21.948231] #Ponta Delgada (SÃ£o Miguel) y Reyjavick\n",
    "outputs_path= \"C:/Users/ideapad 5 15ITL05/Desktop/TFM/Notebooks/Outputs/sst_s_o_n_slp_j/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46da6488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done....... now training the model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                361312    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 875)               56875     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 25, 35)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 419,027\n",
      "Trainable params: 419,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m model1 \u001b[39m=\u001b[39m create_model(input_shape,output_shape, layer_sizes, activations, dropout_rates, kernel_regularizer)\n\u001b[0;32m     10\u001b[0m model1\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearning_rate), loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mMeanSquaredError())\n\u001b[1;32m---> 11\u001b[0m record1\u001b[39m=\u001b[39m model1\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(X_train), np\u001b[39m.\u001b[39;49marray(Y_train),epochs\u001b[39m=\u001b[39;49mepochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(np\u001b[39m.\u001b[39;49marray(X_valid), np\u001b[39m.\u001b[39;49marray(Y_valid)))\u001b[39m#,callbacks=[callback])\u001b[39;00m\n\u001b[0;32m     12\u001b[0m performance_plot(record1)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEvaluate on test data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1607\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1608\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1609\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1610\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1611\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1612\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1613\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1614\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1615\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1616\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1617\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1618\u001b[0m )\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\keras\\engine\\training.py:1939\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1937\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_counter\u001b[39m.\u001b[39massign(\u001b[39m0\u001b[39m)\n\u001b[0;32m   1938\u001b[0m callbacks\u001b[39m.\u001b[39mon_test_begin()\n\u001b[1;32m-> 1939\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   1941\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\keras\\engine\\data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1307\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1308\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1309\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    498\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    698\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    717\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource \u001b[39m=\u001b[39m (\n\u001b[0;32m    718\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39manonymous_iterator_v3(\n\u001b[0;32m    719\u001b[0m           output_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types,\n\u001b[0;32m    720\u001b[0m           output_shapes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 721\u001b[0m   gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\tfm_0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3407\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3408\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3409\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3410\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3411\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "era_slp_regrid,lat_slp,lon_slp = data_mining('Datasets/'+path_era_slp, lat_lims_slp, lon_lims_slp, time_lims, scale=100, rotate_180=True, regrid=True, regrid_degree=2,overlapping=False,variable_name='msl')\n",
    "had_sst_regrid,lat_sst,lon_sst = data_mining('Datasets/'+path_had_sst, lat_lims_sst, lon_lims_sst, time_lims, scale=1, rotate_360=True, regrid=False, regrid_degree=2,overlapping=True,variable_name='sst')\n",
    "data_slp,anom_slp,norm_slp,mean_slp,std_slp= data_selection(era_slp_regrid, months_slp, years,months_skip_slp,years_finally_slp,reference_period)\n",
    "data_sst,anom_sst,norm_sst,mean_sst,std_sst= data_selection(had_sst_regrid, months_sst, years,months_skip_sst,years_finally_slp,reference_period)\n",
    "X_train,X_valid,X_test,Y_train,Y_valid,Y_test= data_split(train_years,validation_years,testing_years,norm_sst,norm_slp, predictor_nans=True, predictant_nans=False)\n",
    "print('Preprocessing done....... now training the model')\n",
    "input_shape,output_shape = X_train.shape[1], Y_train.shape[1:3]\n",
    "#callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "model1 = create_model(input_shape,output_shape, layer_sizes, activations, dropout_rates, kernel_regularizer)\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.MeanSquaredError())\n",
    "record1= model1.fit(np.array(X_train), np.array(Y_train),epochs=epochs, verbose=0, validation_data=(np.array(X_valid), np.array(Y_valid)))#,callbacks=[callback])\n",
    "performance_plot(record1)\n",
    "print(\"Evaluate on test data\")\n",
    "predicted1,correct_value= evaluation(model1,X_test,Y_test,lon_slp,lat_slp,std_slp)\n",
    "X_test_anom= data_sst.sel(year=slice(str(testing_years[0]),str(testing_years[1])))-data_sst.sel(year=slice(str(reference_period[0]),str(reference_period[1]))).mean(dim='year') #desestandarizamos\n",
    "for i in range(1995,2011,2):\n",
    "    year=i\n",
    "    figura= outputs_path+'Comparison'+str(year)+'.png'\n",
    "    DibujoMapados(lon_slp,lat_slp,lon_sst, lat_sst, X_test_anom.sel(year=year),predicted1.sel(time=year),correct_value.sel(year=year),mapbar,unidades,titulo1,titulo2,subtitulo1,periodo,subtitulo2,figura,year)\n",
    "nino3= nino_index(lon_nino3,lat_nino3,xr.open_dataset('Datasets/'+path_had_sst),time_period=[1995,2009])\n",
    "nao_index= nao_index(Azores,icelandia,xr.open_dataset('Datasets/'+path_era_slp),time_period=[1995,2010])\n",
    "nao_index= nao_index.groupby('time.month')\n",
    "nao_index_january, nao_index_february, nao_index_december = nao_index[1],nao_index[2],nao_index[12]\n",
    "nao_index_djf= (np.array(nao_index_january.msl.drop_sel(time='1995-01'))+np.array(nao_index_february.msl.drop_sel(time='1995-02')))/2#+np.array(nao_index_december.msl.drop_sel(time='2010-12')))/3\n",
    "correlations(predicted1,correct_value,outputs_path,lat_iceland,lon_iceland,lat_macaronesian,lon_macaronesian,time_series=True,index_1=nino3.sst,index_1_name='NiÃ±o 3',index_2=nao_index_djf,index_2_name='NAO')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5708259ad43feab1c3c96eaa2f68ed743ff0f29beb40edabae7ec6cebb1611c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
